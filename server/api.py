import os
from fastapi import FastAPI, File, UploadFile
from typing import List
from pydantic import BaseModel

from fastapi.middleware.cors import CORSMiddleware

from pathlib import Path
from typing import Union

from fastapi.responses import FileResponse
from starlette.requests import Request
from starlette.staticfiles import StaticFiles
from starlette.templating import Jinja2Templates
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
tokenize = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")


from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers import pipeline
from langchain_core.output_parsers import StrOutputParser


text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenize,
    task="text-generation",
    temperature=0.1,
    do_sample=True,
    max_new_tokens=500,
)

llm = HuggingFacePipeline(pipeline=text_generation_pipeline)


prompt_template = """
<|system|>
You are an assistant that is able to answer user questions. You may use the context in order to answer the question.

<context>
{context}
</context>

</system>
<|user|>
{question}
</user>

<response>
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

llm_chain = prompt | llm | StrOutputParser()










from langchain.docstore.document import Document
import pandas as pd
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceBgeEmbeddings

df = pd.read_csv("./server/cities.csv")
docs = []
for idx, i in df.iterrows():
    docs.append(
        Document(
            page_content=f"{i["description"]}",
            metadata={"source":"local", "city":i["city"], "description": i["description"]}
        )
    )
    docs.append(
        Document(
            page_content=f"{i["city"]}",
           metadata={"source":"local", "city":i["city"], "description": i["description"]}
        )
    )
    
db = FAISS.from_documents(docs, HuggingFaceBgeEmbeddings(model_name="BAAI/bge-base-en-v1.5"))


retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})




PATH_TO_VUE_APP_BUILD_DIR = f"{os.path.dirname(os.path.realpath(__file__))}/dist"
# For these files on root level, index.html fallback should not happen
STATIC_TOP_LEVEL_FILES = [
    "favicon.ico",
    "manifest.json",
    "logo192.png",
    "logo512.png",
    "robots.txt",
]

app = FastAPI()

origins = [
    "http://localhost:3000",
    "*"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # Allows specified origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)


@app.post("/submit_feedback/")
async def submit_feedback():
   
    return {"response": "Done"}


class Message(BaseModel):
    user_message: str
    chat_history: list


# Create a POST endpoint
@app.post("/submit_messages/")
async def submit_messages(message: Message):
    # Process the message (example)
    
    docs = retriever.get_relevant_documents(message.user_message)
    context = [i.page_content for i in docs]

    response = llm_chain.invoke({"context": context, "question": message.user_message})
    response = response.split("<response>")[-1]
    
    return {"response": response, "reference": [{"source":i, "link": i} for i in context]}


# https://stackoverflow.com/a/70065066/3652805
def serve_react_app_wrapper(outer_app: FastAPI, build_dir: Union[Path, str]) -> FastAPI:
    """Serves a React application in the root directory `/`

    Args:
        outer_app: FastAPI application instance
        build_dir: React build directory (generated by `yarn build` or
            `npm run build`)

    Returns:
        FastAPI: instance with the react application added
    """
    if isinstance(build_dir, str):
        build_dir = Path(build_dir)

    app.mount(
        "/",
        StaticFiles(directory=PATH_TO_VUE_APP_BUILD_DIR, html=True),
        name="VueApp"
    )

    outer_app.mount(
        "/",
        StaticFiles(directory=PATH_TO_VUE_APP_BUILD_DIR, html=True),
        name="VueApp"
    )
    templates = Jinja2Templates(directory=build_dir.as_posix())

    @outer_app.get("/{full_path:path}", include_in_schema=False)
    async def serve_react_app(
        request: Request, full_path: str
    ):  # pylint: disable=unused-argument
        """Serve the react app
        `full_path` variable is necessary to serve each possible endpoint with
        `index.html` file in order to be compatible with `react-router-dom`
        """
        if full_path in STATIC_TOP_LEVEL_FILES:
            return FileResponse(build_dir / full_path)
        return templates.TemplateResponse("index.html", {"request": request})

    return outer_app


app = serve_react_app_wrapper(app, PATH_TO_VUE_APP_BUILD_DIR)